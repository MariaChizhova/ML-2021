{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This cat does not exist\n__Суммарное количество баллов: 10__\n\n__Решение отправлять на `ml.course.practice@gmail.com`__\n\n__Тема письма: `[HSE][ML][HW06] <ФИО>`, где вместо `<ФИО>` указаны фамилия и имя__\n\nЦель этого задания - создать котов, которых не существует. В ходе данного задания вы обучите DCGAN и VAE, которые являются одними из первых генеративных моделей. Для этого задания вам наверняка потребуется GPU с CUDA, поэтому рекомендуется использовать Google Colab.","metadata":{"_uuid":"70076784-7928-428c-ab1f-22de84ece2ee","_cell_guid":"e96eb542-583c-45f7-8e17-9a21b2ae5618","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport os \nimport cv2\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport random\nimport numpy as np\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-11-18T08:49:52.984116Z","iopub.execute_input":"2021-11-18T08:49:52.984393Z","iopub.status.idle":"2021-11-18T08:49:52.99362Z","shell.execute_reply.started":"2021-11-18T08:49:52.984365Z","shell.execute_reply":"2021-11-18T08:49:52.992423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def random_noise(batch_size, channels, side_size):\n    return torch.randn(batch_size, channels, side_size, side_size).cuda()\n\ndef imagewide_average(x):\n    return x.mean(dim=(-1, -2))","metadata":{"execution":{"iopub.status.busy":"2021-11-18T08:49:56.070859Z","iopub.execute_input":"2021-11-18T08:49:56.071667Z","iopub.status.idle":"2021-11-18T08:49:56.077972Z","shell.execute_reply.started":"2021-11-18T08:49:56.071633Z","shell.execute_reply":"2021-11-18T08:49:56.076613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualise(imgs, rows=2):\n    imgs = (imgs.transpose(1, 3) + 1) / 2\n    imgs = torch.cat([imgs[i::rows] for i in range(rows)], dim=1)\n    cols = len(imgs)\n    imgs = (torch.cat(list(imgs), dim=1)).cpu().numpy()[:, :, ::-1]\n    plt.figure(figsize=(cols*1.5, rows*1.5))\n    plt.imshow(imgs)\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T08:49:58.271375Z","iopub.execute_input":"2021-11-18T08:49:58.272352Z","iopub.status.idle":"2021-11-18T08:49:58.283337Z","shell.execute_reply.started":"2021-11-18T08:49:58.272302Z","shell.execute_reply":"2021-11-18T08:49:58.281888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CatDataset(Dataset):\n    def __init__(self, path_to_dataset=\"../input/catsforgan/cat_136\", size=64):\n        self.photo_names = os.listdir(path_to_dataset)\n        self.path_base = path_to_dataset\n        self.size = size\n    \n    def __getitem__(self, index):\n        path = self.path_base + \"/\" + self.photo_names[index]\n        img = cv2.imread(path) # 136 x 136\n        crop_rate = 8\n        x_crop = random.randint(0, crop_rate)\n        y_crop = random.randint(0, crop_rate)\n        img = img[x_crop:136 - crop_rate + x_crop, y_crop:136 - crop_rate + y_crop]\n        img = cv2.resize(img, (self.size, self.size), interpolation=cv2.INTER_CUBIC)\n        return 2 * torch.tensor(img).float().transpose(0, 2) / 255. - 1\n    \n    def __len__(self):\n        return len(self.photo_names)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T08:50:01.211141Z","iopub.execute_input":"2021-11-18T08:50:01.211457Z","iopub.status.idle":"2021-11-18T08:50:01.224808Z","shell.execute_reply.started":"2021-11-18T08:50:01.211402Z","shell.execute_reply":"2021-11-18T08:50:01.22379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = CatDataset()\nvisualise(torch.cat([dataset[i].unsqueeze(0) for i in [3, 15, 182, 592, 394, 2941]], dim=0))","metadata":{"execution":{"iopub.status.busy":"2021-11-18T08:50:03.891876Z","iopub.execute_input":"2021-11-18T08:50:03.892509Z","iopub.status.idle":"2021-11-18T08:50:04.32936Z","shell.execute_reply.started":"2021-11-18T08:50:03.892474Z","shell.execute_reply":"2021-11-18T08:50:04.327597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Задание 1 (2 балла)\nДля начала реализуем генератор для нашего DCGAN. Предлагается использовать следующую архитектуру:\n\n![](https://drive.google.com/uc?export=view&id=1ymxDjbVTDCtm2xaoSpaRQIgXJyDIdPgp)\n\nДля ее реализации вам потребуются модули `nn.BatchNorm2d`, `nn.Conv2d`, `nn.ConvTranspose2D`, `nn.ReLU`, а также функция `F.interpolate`.\n\n#### Методы\n* `__init__` - принимает на вход `start_size`, `latent_channels`, `start_channels` и `upsamplings`. Первые два аргумента отвечают за размер случайного шума, из которого в последствии будет сгенерирована картинка. `start_channels` отвечает за то, сколько каналов должно быть в картинке перед тем, как к ней будут применены upsampling блоки. `upsamplings` - это количество upsampling блоков, которые должны быть применены к картинке. В каждом таком локе количество каналов уменьшается в два раза.\n\n\n* `forward` - принимает на вход `batch_size`, генерирует `batch_size` картинок из случайного шума.","metadata":{}},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, start_size=2, latent_channels=32, start_channels=1024, upsamplings=6):\n        super().__init__()\n        self.start_size = start_size\n        self.latent_channels = latent_channels\n        self.start_channels = c = start_channels\n        self.upsamplings = upsamplings\n        self.conv1 = nn.Conv2d(in_channels=self.latent_channels, out_channels=c, kernel_size=1, stride=1, padding=0, bias=False)\n        self.convTranspose = nn.ModuleList()\n        self.BatchNorm = nn.ModuleList()\n        self.ReLU = nn.ModuleList()\n        for _ in range(self.upsamplings):\n            out = self.convTranspose.append(nn.ConvTranspose2d(in_channels=c, out_channels=c // 2, kernel_size=4, stride=2, padding=1, bias=False))\n            out = self.BatchNorm.append(nn.BatchNorm2d(c // 2))\n            out = self.ReLU.append(nn.ReLU())\n            c //= 2\n        self.conv2 = nn.Conv2d(in_channels=c, out_channels=3, kernel_size=1, stride=1, padding=0, bias=False)\n        self.tanh = nn.Tanh()\n    \n    def forward(self, batch_size: int):\n        out = random_noise(batch_size, self.latent_channels, self.start_size)\n        out = self.conv1(out)\n        for i in range(self.upsamplings):\n            out = self.convTranspose[i](out)\n            out = self.BatchNorm[i](out)\n            out = self.ReLU[i](out)\n        out = self.conv2(out)\n        out = self.tanh(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2021-11-18T08:50:07.692371Z","iopub.execute_input":"2021-11-18T08:50:07.692673Z","iopub.status.idle":"2021-11-18T08:50:07.70767Z","shell.execute_reply.started":"2021-11-18T08:50:07.692643Z","shell.execute_reply":"2021-11-18T08:50:07.70618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Задание 2 (2 балла)\nДля начала реализуем дискриминатор для нашего DCGAN. Предлагается использовать следующую архитектуру:\n\n![](https://drive.google.com/uc?export=view&id=1x8SSWEWozo9bmsOvXVHUCk97Dv5MCx-G)\n\n\nДля ее реализации вам потребуются модули `nn.BatchNorm2d`, `nn.Conv2d`, `nn.ReLU` и `nn.Sigmoid`.\n\n#### Методы\n* `__init__` - принимает на вход `start_channels` и `downsamplings`. `start_channels` определяет количество каналов, которые должны быть в изображении перед применением downsampling блоков.\n\n\n* `forward` - принимает на вход `x` - тензор с картинками. Возвращает вектор с размерностью `batch_size`.","metadata":{}},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, downsamplings=6, start_channels=8):\n        super().__init__()\n        self.image_size = 128\n        self.downsamplings = downsamplings\n        h = w = self.image_size // (2 ** self.downsamplings)\n        self.start_channels = start_channels\n        c = self.start_channels\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=c, kernel_size=1, stride=1, padding=0, bias=False)\n        self.conv2 = nn.ModuleList()\n        self.BatchNorm = nn.ModuleList()\n        self.ReLU = nn.ModuleList()\n        for _ in range(self.downsamplings):\n            self.conv2.append(nn.Conv2d(in_channels=c, out_channels=c * 2, kernel_size=3, stride=2, padding=1,  bias=False))\n            self.BatchNorm.append(nn.BatchNorm2d(c * 2))\n            self.ReLU.append(nn.ReLU())\n            c *= 2\n        self.flatten = nn.Flatten()\n        self.linear = nn.Linear(in_features=c * w * h, out_features=1, bias=False)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        out = self.conv1(x)\n        for i in range(self.downsamplings):\n            out = self.conv2[i](out)\n            out = self.BatchNorm[i](out)\n            out = self.ReLU[i](out)\n        out = self.flatten(out)\n        out = self.linear(out)\n        out = self.sigmoid(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2021-11-18T08:50:11.150953Z","iopub.execute_input":"2021-11-18T08:50:11.151269Z","iopub.status.idle":"2021-11-18T08:50:11.163147Z","shell.execute_reply.started":"2021-11-18T08:50:11.15124Z","shell.execute_reply":"2021-11-18T08:50:11.162219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_gan():\n    generator = Generator()\n    discriminator = Discriminator()\n    epochs = 120\n    visualise_every = 40\n    batch_size = 8\n    generator.cuda()\n    discriminator.cuda()\n\n    gen_optim = Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n    disc_optim = Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n\n    dataset = CatDataset(size=128)\n\n    for ep in range(epochs):\n        dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n        total_batches = 0\n        gen_loss_avg = 0\n        disc_loss_avg = 0\n\n        for i, batch in tqdm(enumerate(dataloader), total=(len(dataset) + batch_size) // batch_size):\n            if len(batch) < batch_size:\n                continue\n            total_batches += 1\n            # Positive update\n            batch = batch.cuda()\n            pred = discriminator(batch)\n            loss = F.binary_cross_entropy(pred, torch.ones_like(pred))\n            disc_optim.zero_grad()\n            loss.backward()\n            disc_optim.step()\n            disc_loss_avg += loss.item()\n\n            # Negative update\n            batch = generator(batch_size).detach()\n            pred = discriminator(batch)\n            loss = F.binary_cross_entropy(pred, torch.zeros_like(pred))\n            disc_optim.zero_grad()\n            loss.backward()\n            disc_optim.step()\n            disc_loss_avg += loss.item()\n\n            # Generator update\n            batch = generator(batch_size)\n            pred = discriminator(batch)\n            loss = F.binary_cross_entropy(pred, torch.ones_like(pred))\n            gen_optim.zero_grad()\n            loss.backward()\n            gen_optim.step()\n            gen_loss_avg += loss.item()\n            \n            if (ep + 1) % visualise_every == 0:\n                with torch.no_grad():\n                    visualise(generator(6), rows=2)\n\n        print(f\"Epoch {ep+1} | Discriminator loss: {disc_loss_avg / total_batches} | Generator loss: {gen_loss_avg / total_batches}\")","metadata":{"execution":{"iopub.status.busy":"2021-11-18T08:50:13.753325Z","iopub.execute_input":"2021-11-18T08:50:13.753706Z","iopub.status.idle":"2021-11-18T08:50:13.768811Z","shell.execute_reply.started":"2021-11-18T08:50:13.753674Z","shell.execute_reply":"2021-11-18T08:50:13.767657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_gan()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T08:50:16.449514Z","iopub.execute_input":"2021-11-18T08:50:16.450515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Задание 3 (5 баллов)\nТеперь посмотрим на другую модель: Variational Autoencoder. В отличии от GAN, в котором генератор пытается себя обмануть дискриминатор, а дискриминатор старается не быть обманутым, VAE решает задачу реконструкции элемента множества X с применением регуляризации в латентном пространстве. \n\nПолностью архитектура выглядит так:\n![](https://drive.google.com/uc?export=view&id=1NcbHEq4_4mBpkWngUYD86hX45-ou7nU_)\n\nИз нее можно выделить две части: Encoder (по изображению возвращает mu и sigma) и Decoder (по случайному шуму восстанавливает изображение). На высоком уровне VAE можно представить так:\n\n![](https://drive.google.com/uc?export=view&id=1soxN7o1oFW07kLoIE0tZ9hK_XKqfVgqa)\n\nВ данном задании вам необходимо реализовать полную архитектуру VAE.\n\n#### Методы\n* `__init__` - принимает на вход `img_size`, `downsamplings`, `latent_size`, `linear_hidden_size`, `down_channels` и `up_channels`. `img_size` - размер стороны входного изображения. `downsamplings` - количество downsampling (и upsampling) блоков. `latent_size` - размер латентного пространства, в котором в который будет закодирована картинка. `linear_hidden_size` количество нейронов на скрытом слое полносвязной сети в конце encoder'а. Для полносвязной сети decoder'а это число стоит умножить на 2. `down_channels` - количество каналов, в которое будет преобразовано трехцветное изображение перед применением `downsampling` блоков. `up_channels` - количество каналов, которое должно получиться после применения всех upsampling блоков.\n\n* `forward` - принимает на вход `x`. Считает распределение $N(\\mu, \\sigma^2)$ и вектор $z \\sim N(\\mu, \\sigma^2)$. Возвращает $x'$ - восстановленную из вектора $z$ картинку и $D_{KL}(N(\\mu, \\sigma^2), N(0, 1)) = 0.5 \\cdot (\\sigma^2 + \\mu^2 - \\log \\sigma^2 - 1)$.\n\n* `encode` - принимает на вход `x`. Возвращает вектор из распределения $N(\\mu, \\sigma^2)$.\n\n* `decode` - принимает на вход `z`. Возвращает восстановленную по вектору картинку.\n\n\n#### Если хочется улучшить качество\nhttps://arxiv.org/pdf/1906.00446.pdf","metadata":{"execution":{"iopub.status.busy":"2021-11-16T20:43:29.144356Z","iopub.status.idle":"2021-11-16T20:43:29.144791Z","shell.execute_reply.started":"2021-11-16T20:43:29.144548Z","shell.execute_reply":"2021-11-16T20:43:29.144572Z"}}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, downsamplings, latent_size, down_channels):\n        super().__init__()\n        self.downsamplings = downsamplings\n        self.latent_size = latent_size\n        self.down_channels = c = down_channels\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=c, kernel_size=1, stride=1, padding=0)\n        self.conv2 = nn.ModuleList()\n        self.BatchNorm = nn.ModuleList()\n        self.ReLU = nn.ModuleList()\n        for _ in range(self.downsamplings):\n            self.conv2.append(nn.Conv2d(in_channels=c, out_channels=c * 2, kernel_size=3, stride=2, padding=1))\n            self.BatchNorm.append(nn.BatchNorm2d(c * 2))\n            self.ReLU.append(nn.ReLU())\n            c *= 2\n        self.conv3 = nn.Conv2d(in_channels=c, out_channels=2 * self.latent_size, kernel_size=1, stride=1, padding=0)\n        \n    def forward(self, x):\n        out = self.conv1(x)\n        for i in range(self.downsamplings):\n            out = self.conv2[i](out)\n            out = self.BatchNorm[i](out)\n            out = self.ReLU[i](out)\n        out = self.conv3(out)\n        self.mu, self.sigma = torch.split(out, self.latent_size, dim=1)\n        self.sigma = torch.exp(self.sigma)\n        return self.mu + torch.randn_like(self.sigma) * self.sigma","metadata":{"execution":{"iopub.status.busy":"2021-11-17T18:54:47.109496Z","iopub.execute_input":"2021-11-17T18:54:47.109787Z","iopub.status.idle":"2021-11-17T18:54:47.124161Z","shell.execute_reply.started":"2021-11-17T18:54:47.109754Z","shell.execute_reply":"2021-11-17T18:54:47.123042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, upsamplings, latent_size, up_channels):\n        super().__init__()\n        self.upsamplings = upsamplings\n        self.latent_size = latent_size\n        self.up_channels = c = up_channels * 2 ** self.upsamplings\n        self.conv1 = nn.Conv2d(in_channels=self.latent_size, out_channels=c, kernel_size=1, stride=1, padding=0)\n        self.convTranspose = nn.ModuleList()\n        self.BatchNorm = nn.ModuleList()\n        self.ReLU = nn.ModuleList()\n        for _ in range(self.upsamplings):\n            self.convTranspose.append(nn.ConvTranspose2d(in_channels=c, out_channels=c // 2, kernel_size=4, stride=2, padding=1))\n            self.BatchNorm.append(nn.BatchNorm2d(c // 2))\n            self.ReLU.append(nn.ReLU())\n            c //= 2\n        self.conv3 = nn.Conv2d(in_channels=c, out_channels=3, kernel_size=1, stride=1, padding=0)\n        self.tanh = nn.Tanh()\n        \n    def forward(self, x):\n        out = self.conv1(x)\n        for i in range(self.upsamplings):\n            out = self.convTranspose[i](out)\n            out = self.BatchNorm[i](out)\n            out = self.ReLU[i](out)\n        out = self.conv3(out)\n        out = self.tanh(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2021-11-17T18:55:37.604325Z","iopub.execute_input":"2021-11-17T18:55:37.604622Z","iopub.status.idle":"2021-11-17T18:55:37.617281Z","shell.execute_reply.started":"2021-11-17T18:55:37.604589Z","shell.execute_reply":"2021-11-17T18:55:37.616124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VAE(nn.Module):\n    def __init__(self, downsamplings=7, latent_size=512, down_channels=64, up_channels=16):\n        super().__init__()\n        self.encoder = Encoder(downsamplings, latent_size, down_channels)\n        self.decoder = Decoder(downsamplings, latent_size, up_channels)\n        \n    def forward(self, x):\n        x_pred = self.decode(self.encode(x))\n        kld = 0.5 * (self.encoder.sigma ** 2 + self.encoder.mu ** 2 -  torch.log(self.encoder.sigma ** 2) - 1)\n        return x_pred, kld\n    \n    def encode(self, x):\n        return self.encoder.forward(x)\n    \n    def decode(self, z):\n        return self.decoder.forward(z)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T18:55:40.19446Z","iopub.execute_input":"2021-11-17T18:55:40.194755Z","iopub.status.idle":"2021-11-17T18:55:40.204202Z","shell.execute_reply.started":"2021-11-17T18:55:40.194721Z","shell.execute_reply":"2021-11-17T18:55:40.202807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_vae():\n    vae = VAE()\n    vae.cuda()\n\n    epochs = 201\n    batch_size = 8\n    vae_optim = Adam(vae.parameters(), lr=1e-4)\n\n    dataset = CatDataset(size=128)\n\n    test_imgs_1 = torch.cat([dataset[i].unsqueeze(0) for i in (0, 34, 76, 1509)])\n    test_imgs_2 = torch.cat([dataset[i].unsqueeze(0) for i in (734, 123, 512, 3634)])\n\n    for ep in range(epochs):\n        dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n        total_batches = 0\n        rec_loss_avg = 0\n        kld_loss_avg = 0\n\n        if ep % 10 == 0:\n            with torch.no_grad():\n                z_1 = vae.encode(test_imgs_1.cuda())\n                z_2 = vae.encode(test_imgs_2.cuda())\n                x_int = []\n                for i in range(9):\n                    z = (i * z_1 + (8 - i) * z_2) / 8\n                    x_int.append(vae.decode(z))\n                x_int = torch.cat(x_int)\n                visualise(x_int, rows=len(test_imgs_1))\n                z_rand = torch.randn_like(z_1)\n                x_int = vae.decode(z_rand)\n                visualise(x_int, rows=len(test_imgs_1)//2)\n\n        for i, batch in tqdm(enumerate(dataloader), total=(len(dataset) + batch_size) // batch_size):\n            if len(batch) < batch_size:\n                continue\n            total_batches += 1\n            x = batch.cuda()\n            x_rec, kld = vae(x)\n            img_elems = float(np.prod(list(batch.size())))\n            kld_loss = kld.sum() / batch_size\n            rec_loss = ((x_rec - x)**2).sum() / batch_size\n            loss = rec_loss + 0.1 * kld_loss # https://openreview.net/forum?id=Sy2fzU9gl\n            vae_optim.zero_grad()\n            loss.backward()\n            vae_optim.step()\n            kld_loss_avg += kld_loss.item()\n            rec_loss_avg += rec_loss.item()\n\n        print(f\"Epoch {ep+1} | Reconstruction loss: {rec_loss_avg / total_batches} | KLD loss: {kld_loss_avg / total_batches}\")\n\ntrain_vae()","metadata":{"execution":{"iopub.status.busy":"2021-11-17T18:55:42.758951Z","iopub.execute_input":"2021-11-17T18:55:42.759554Z","iopub.status.idle":"2021-11-18T01:33:44.785956Z","shell.execute_reply.started":"2021-11-17T18:55:42.759516Z","shell.execute_reply":"2021-11-18T01:33:44.784645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}